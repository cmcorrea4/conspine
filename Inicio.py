import streamlit as st
from pinecone import Pinecone
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import ChatPromptTemplate

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="Consulta de Base de Datos Vectorial", layout="wide")
st.title("üîç Sistema de Consulta Inteligente con Pinecone")

# Funci√≥n para obtener √≠ndices de Pinecone
def get_pinecone_indexes(api_key):
    try:
        pc = Pinecone(api_key=api_key)
        current_indexes = pc.list_indexes().names()
        return list(current_indexes)
    except Exception as e:
        st.error(f"Error al obtener √≠ndices: {str(e)}")
        return []

# Funci√≥n para limpiar estados
def clear_all_states():
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.experimental_rerun()

# Inicializaci√≥n de estados
if 'system_initialized' not in st.session_state:
    st.session_state.system_initialized = False

# Sidebar para configuraci√≥n
with st.sidebar:
    st.markdown("### ‚öôÔ∏è Configuraci√≥n")
    
    # Campo para OpenAI API Key
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="Introduce tu API key de OpenAI"
    )
    
    # Campo para Pinecone API Key
    pinecone_api_key = st.text_input(
        "Pinecone API Key",
        type="password",
        help="Introduce tu API key de Pinecone"
    )
    
    # Selector de modelo LLM
    llm_model = st.selectbox(
        "Modelo LLM",
        options=["gpt-3.5-turbo", "gpt-4o-mini"],
        help="Selecciona el modelo de lenguaje a utilizar"
    )
    
    # Temperatura del modelo
    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.3,
        help="Controla la creatividad de las respuestas"
    )
    
    # Botones de control
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üîÑ Actualizar"):
            st.experimental_rerun()
    with col2:
        if st.button("üóëÔ∏è Limpiar"):
            clear_all_states()
    
    # Verificar conexi√≥n con Pinecone y mostrar √≠ndices
    if pinecone_api_key:
        try:
            st.markdown("### üìä Estado")
            available_indexes = get_pinecone_indexes(pinecone_api_key)
            
            if available_indexes:
                st.success("‚úÖ Conectado a Pinecone")
                
                # Selector de √≠ndice
                selected_index = st.selectbox(
                    "Selecciona un √≠ndice",
                    options=available_indexes
                )
                
                # Mostrar informaci√≥n del √≠ndice seleccionado
                if selected_index:
                    pc = Pinecone(api_key=pinecone_api_key)
                    index = pc.Index(selected_index)
                    stats = index.describe_index_stats()
                    
                    # Mostrar estad√≠sticas b√°sicas
                    st.markdown("#### üìà Estad√≠sticas")
                    total_vectors = stats.get('total_vector_count', 0)
                    st.metric("Total de vectores", total_vectors)
                    
                    # Mostrar namespaces disponibles
                    if 'namespaces' in stats:
                        st.markdown("#### üè∑Ô∏è Namespaces")
                        namespaces = list(stats['namespaces'].keys())
                        if namespaces:
                            selected_namespace = st.selectbox(
                                "Selecciona un namespace",
                                options=namespaces
                            )
                            st.session_state.namespace = selected_namespace
                        else:
                            st.info("No hay namespaces disponibles")
                            st.session_state.namespace = ""
            else:
                st.warning("‚ö†Ô∏è No hay √≠ndices disponibles")
                selected_index = None
                
        except Exception as e:
            st.error(f"‚ùå Error de conexi√≥n: {str(e)}")
            selected_index = None
    else:
        selected_index = None

def get_enhanced_response(query, context_results, llm):
    """Genera una respuesta mejorada usando el modelo LLM."""
    # Template para el prompt
    template = """
    Act√∫a como un asistente experto y √∫til. Bas√°ndote en la siguiente informaci√≥n y pregunta,
    genera una respuesta, breve, clara, precisa y bien estructurada, si no hay contexto, di que no tienes contexto.

    Pregunta: {query}

    Contexto relevante:
    {context}

    Por favor, proporciona una respuesta que:
    1. Sea relevante y directa
    2. Est√© bien estructurada y sea f√°cil de entender
    3. Incluya ejemplos o referencias cuando sea apropiado
    4. Mantenga un tono profesional pero amigable

    Respuesta:
    """

    # Preparar el contexto
    context_texts = []
    for match in context_results.matches:
        if 'text' in match.metadata:
            context_texts.append(f"- {match.metadata['text']}")
    
    context = "\n".join(context_texts)
    
    # Crear el prompt
    prompt = ChatPromptTemplate.from_template(template)
    
    # Generar la respuesta
    messages = prompt.format_messages(
        query=query,
        context=context
    )
    
    response = llm.invoke(messages)
    return response.content

def query_pinecone(query_text, namespace, k=5):
    try:
        # Inicializar embeddings y LLM
        embedding_model = OpenAIEmbeddings(
            openai_api_key=openai_api_key,
            model="text-embedding-ada-002"
        )
        
        llm = ChatOpenAI(
            temperature=temperature,
            model_name=llm_model,
            openai_api_key=openai_api_key
        )
        
        # Generar embedding para la consulta
        query_embedding = embedding_model.embed_query(query_text)
        
        # Inicializar Pinecone
        pc = Pinecone(api_key=pinecone_api_key)
        index = pc.Index(selected_index)
        
        # Realizar b√∫squeda
        results = index.query(
            vector=query_embedding,
            namespace=namespace,
            top_k=k,
            include_metadata=True
        )
        
        # Generar respuesta mejorada
        enhanced_response = get_enhanced_response(query_text, results, llm)
        
        return results, enhanced_response
        
    except Exception as e:
        st.error(f"Error en la consulta: {str(e)}")
        return None, None

# Interface principal
if openai_api_key and pinecone_api_key and selected_index:
    st.markdown("### üîç Realizar Consulta")
    
    # Par√°metros de b√∫squeda
    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input("üí≠ ¬øQu√© deseas consultar?")
    with col2:
        k = st.number_input("N√∫mero de resultados", min_value=1, max_value=10, value=5)
    
    # Bot√≥n de b√∫squeda
    if st.button("üîç Buscar"):
        if query:
            with st.spinner("üîÑ Buscando y procesando..."):
                results, enhanced_response = query_pinecone(
                    query,
                    namespace=getattr(st.session_state, 'namespace', ''),
                    k=k
                )
                
                if results and hasattr(results, 'matches'):
                    # Mostrar respuesta mejorada
                    st.markdown("### ü§ñ Respuesta Generada:")
                    st.write(enhanced_response)
                    
                    # Mostrar fuentes
                    st.markdown("### üìö Fuentes Consultadas:")
                    
                    for i, match in enumerate(results.matches, 1):
                        score = match.score
                        similarity = round((1 - (1 - score)) * 100, 2)
                        
                        with st.expander(f"üìç Fuente {i} - Similitud: {similarity}%"):
                            if 'text' in match.metadata:
                                st.write(match.metadata['text'])
                            else:
                                st.write("No se encontr√≥ texto en los metadatos")
                            
                            # Mostrar metadatos adicionales
                            other_metadata = {k:v for k,v in match.metadata.items() if k != 'text'}
                            if other_metadata:
                                st.markdown("##### Metadatos adicionales:")
                                st.json(other_metadata)
                else:
                    st.warning("No se encontraron resultados")
        else:
            st.warning("‚ö†Ô∏è Por favor, ingresa una consulta")
else:
    st.info("üëà Por favor, configura las credenciales en el panel lateral para comenzar")

# Informaci√≥n en el sidebar
with st.sidebar:
    st.markdown("---")
    st.markdown("### ‚ÑπÔ∏è Sobre esta aplicaci√≥n")
    st.write("""
    Esta aplicaci√≥n te permite realizar consultas sem√°nticas mejoradas con IA en bases de datos
    vectoriales existentes en Pinecone.
    
    Caracter√≠sticas:
    - Conexi√≥n directa a √≠ndices de Pinecone
    - B√∫squeda sem√°ntica con OpenAI
    - Procesamiento con LLM para mejorar respuestas
    - Soporte para m√∫ltiples namespaces
    - Visualizaci√≥n de similitud y fuentes
    """)
